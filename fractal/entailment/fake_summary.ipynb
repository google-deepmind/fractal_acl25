{
  "cells": [
    {
      "metadata": {
        "id": "OzqBDektByrN"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v2 as tf\n",
        "tf.enable_v2_behavior()\n",
        "import multiprocessing.dummy as multiprocessing\n",
        "from multiprocessing import Lock\n",
        "from tqdm import tqdm\n",
        "from nltk import pos_tag\n",
        "import json\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "nxn5jH7nCQbw"
      },
      "cell_type": "code",
      "source": [
        "rng = np.random.default_rng()\n",
        "\n",
        "llm_model = None\n",
        "# TO-DO: Load the model using the instructions in the link below.\n",
        "# Instructions for using PaLM 2 Bison Chat: https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/chat-bison?inv=1\u0026invt=AbytLg\n",
        "\n",
        "num_replicas = 1 # Parallelization recommended \n",
        "batch_size_per_replica = 16\n",
        "num_threads = batch_size_per_replica * num_replicas\n",
        "MAX_RETRIES = 4\n",
        "\n",
        "pool = multiprocessing.Pool(num_threads)\n",
        "\n",
        "data_dir = '/data/preprocessed_wikicatsum' # Add the correct path (eg: path of the file '{data_dir}/{domain}/fake_sentence_replacement_{split}.tgt.json')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "ntRBGcx2CUk5"
      },
      "cell_type": "code",
      "source": [
        "PROMPT_SENTENCE_REPLACEMENT = '\\n Given this paragraph, predict the next sentence and only print the predicted sentence. Print only the predicted sentence.'\n",
        "def prompt_sentence_replacement(left_context):\n",
        "  return 'You are provided with the following paragraph: \\n' + left_context + PROMPT_SENTENCE_REPLACEMENT\n",
        "\n",
        "PROMPT_WORD_REPLACEMENT = \"\"\"\\n In this paragraph, certain words have been masked. They are indicated as '[MASK]'.\n",
        "Please predict the masked words to complete the paragraph. Each '[MASK]' should be replaced with a single word.\n",
        "Print the paragraph after replacing all the \"[MASK]\" with their respective predicted words. No other text should be printed. Print only the completed paragraph.\n",
        "Make sure no '[MASK]' remains in the output.\"\"\"\n",
        "def prompt_word_replacement(masked_summary):\n",
        "  return 'You are provided with the following paragraph: \\n' + masked_summary + PROMPT_WORD_REPLACEMENT"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "MHih0yWlCZCh"
      },
      "cell_type": "code",
      "source": [
        "# TO-DO: Add function to generate text after passing the prompt to the llm_model\n",
        "\n",
        "def generate_word_replacement_summary(reference_summary_sentence_split):\n",
        "  # Reference summary is already split into sentences\n",
        "  if isinstance(reference_summary_sentence_split, str):\n",
        "    reference_summary_sentence_split = reference_summary_sentence_split.split(' \u003cSNT\u003e ')\n",
        "  reference_summary = ' \u003cSNT\u003e '.join(reference_summary_sentence_split)\n",
        "  reference_summary_word_split = reference_summary.split()\n",
        "  tokens_tag = pos_tag(reference_summary_word_split)\n",
        "  replace_pos = ['VERB','NOUN', 'PROPN', 'NUM', 'VB', 'VBG', 'VBD', 'VBN', 'VBP', 'VBZ', 'NN', 'NNS', 'NNP', 'NNPS', 'CB']\n",
        "  token_idx = []\n",
        "  snt_token_id = []\n",
        "  for t_id in range(len(tokens_tag)):\n",
        "    # print(f'({reference_summary_word_split[t_id]}, {tokens_tag[t_id][1]})')\n",
        "    if reference_summary_word_split[t_id] == '\u003cSNT\u003e':\n",
        "      snt_token_id.append(t_id)\n",
        "    elif tokens_tag[t_id][1] in replace_pos:\n",
        "      token_idx.append(t_id)\n",
        "  snt_token_id.append(len(reference_summary_word_split))\n",
        "  if len(token_idx) \u003e= 5:\n",
        "    num_words_to_replace = rng.integers(1,len(token_idx)//5, endpoint = True)\n",
        "  else:\n",
        "    num_words_to_replace = 1\n",
        "  word_idx_to_replace = np.sort(rng.choice(token_idx, num_words_to_replace, replace = False))\n",
        "  for word_id in word_idx_to_replace:\n",
        "    reference_summary_word_split[word_id] = '[MASK]'\n",
        "  masked_summary = (' '.join(reference_summary_word_split)).split('\u003cSNT\u003e')\n",
        "  predicted_summary = masked_summary\n",
        "\n",
        "  # One call to LLM\n",
        "  sent_idx_replaced = []\n",
        "  predicted = ' \u003cSNT\u003e '.join(predicted_summary)\n",
        "  retries = 0\n",
        "  replaced_word_snt_flag = 0\n",
        "  while ('[MASK]' in predicted or len(sent_idx_replaced) == 0 or replaced_word_snt_flag == 1) and retries \u003c MAX_RETRIES:\n",
        "    retries += 1\n",
        "    sent_idx_replaced = []\n",
        "    prompt = prompt_word_replacement(predicted)\n",
        "    predicted, _ = llm_model.Generate(prompt)[0] # Add the function for generation using LLM\n",
        "    predicted_summary = predicted.split(' \u003cSNT\u003e ')\n",
        "    if (len(predicted_summary) \u003e len(reference_summary_sentence_split) or len(predicted_summary) \u003c len(reference_summary_sentence_split)):\n",
        "      replaced_word_snt_flag = 1\n",
        "    else:\n",
        "      for p in range(len(predicted_summary)):\n",
        "        predicted_sentence_word_split = predicted_summary[p].lower().split()\n",
        "        reference_sentence_word_split = reference_summary_sentence_split[p].lower().split()\n",
        "        if reference_sentence_word_split != predicted_sentence_word_split:\n",
        "          sent_idx_replaced.append(p)\n",
        "  if retries == MAX_RETRIES:\n",
        "    predicted.replace('[MASK]', '')\n",
        "\n",
        "  return predicted_summary, list(set(sent_idx_replaced))\n",
        "\n",
        "# TO-DO: Add function to generate text after passing the prompt to the llm_model\n",
        "\n",
        "def generate_sentence_replacement_summary(reference_summary):\n",
        "  if isinstance(reference_summary, str):\n",
        "    reference_summary = reference_summary.split(' \u003cSNT\u003e ')\n",
        "  # Reference summary is already split into sentences\n",
        "  num_sentences = len(reference_summary)\n",
        "  fake_summary = reference_summary\n",
        "  sent_idx_replaced = np.array([])\n",
        "  if num_sentences \u003e= 2:\n",
        "    num_sentences_replace = rng.integers(1,num_sentences//2, endpoint = True)\n",
        "    sent_idx_replaced = np.sort(rng.choice(np.arange(1, num_sentences), num_sentences_replace, replace = False))\n",
        "    for s in sent_idx_replaced:\n",
        "      left_context = ' '.join(fake_summary[:s])\n",
        "      prompt = prompt_sentence_replacement(left_context)\n",
        "      predicted_sentence, _ = llm_model.Generate(prompt)[0] # Add the function for generation using LLM\n",
        "      fake_summary[s] = predicted_sentence\n",
        "  return fake_summary, sent_idx_replaced.tolist()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "lgPH1UlbCs5p"
      },
      "cell_type": "code",
      "source": [
        "def sample_idx(len_ds, num_split_sample_list):\n",
        "  ds_idx = np.arange(len_ds)\n",
        "  pos_sample_idx = np.sort(rng.choice(ds_idx, num_split_sample_list[0], replace = False))\n",
        "  ds_idx = np.setdiff1d(ds_idx, pos_sample_idx)\n",
        "  sentence_replacement_idx = np.sort(rng.choice(ds_idx, num_split_sample_list[1], replace = False))\n",
        "  ds_idx = np.setdiff1d(ds_idx, sentence_replacement_idx)\n",
        "  word_replacement_idx = np.sort(rng.choice(ds_idx, num_split_sample_list[2], replace = False))\n",
        "  return pos_sample_idx.tolist(), sentence_replacement_idx.tolist(), word_replacement_idx.tolist()\n",
        "\n",
        "def chunker(iterable, chunk_size, fill=None):\n",
        "  return (iterable[pos:pos + chunk_size] for pos in range(0, len(iterable), chunk_size))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Vcm0DIMaCw33"
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "domains = ['film', 'company', 'animal']\n",
        "splits = ['test','valid','train']\n",
        "\n",
        "# num_split_sample = [num_split_pos_sample, num_split_sentence_replacement_sample, num_split_word_replacement_sample]\n",
        "num_sample = {\n",
        "  'train':[7500, 5500, 2000],\n",
        "  'test': [400, 250, 150],\n",
        "  'valid': [400, 250, 150]\n",
        "}\n",
        "\n",
        "for split in splits:\n",
        "  for domain in domains:\n",
        "    sentence_promises = []\n",
        "    word_promises = []\n",
        "    sentence_summaries = []\n",
        "    word_summaries = []\n",
        "    pos_summaries = []\n",
        "    print('domain:', domain, 'split:', split)\n",
        "    sentence_output_file = f'{data_dir}/{domain}/fake_sentence_replacement_{split}.tgt.json'\n",
        "    word_output_file = f'{data_dir}/{domain}/fake_word_replacement_{split}.tgt.json'\n",
        "    pos_output_file = f'{data_dir}/{domain}/pos_{split}.tgt.json'\n",
        "    with open(f'{data_dir}/{domain}/{split}.tgt' , 'r') as f, open(sentence_output_file, 'w') as sw, open(word_output_file, 'w') as ww, open(pos_output_file, 'w') as pw:\n",
        "      f_readlines = f.readlines()\n",
        "      pos_sample_idx, sentence_replacement_idx, word_replacement_idx = sample_idx(len(f_readlines), num_sample[split])\n",
        "      pos_indexed_summaries = [f_readlines[i] for i in pos_sample_idx]\n",
        "      sentence_replacement_indexed_summaries = [f_readlines[i] for i in sentence_replacement_idx]\n",
        "      word_replacement_indexed_summaries = [f_readlines[i] for i in word_replacement_idx]\n",
        "\n",
        "      pos_summaries = [{'ds_sample_id': ind, 'summary':reference_summary, 'sent_idx_replaced': []}\n",
        "                       for ind, reference_summary in zip(pos_sample_idx, pos_indexed_summaries)]\n",
        "\n",
        "      sent_results = []\n",
        "      for reference_summaries in tqdm(chunker(sentence_replacement_indexed_summaries, chunk_size=num_threads), total=len(sentence_replacement_indexed_summaries)/num_threads):\n",
        "        sent_results += pool.map(generate_sentence_replacement_summary, reference_summaries)\n",
        "      sentence_summaries = [{'ds_sample_id': int(ind), 'summary':' \u003cSNT\u003e '.join(fake_summary), 'sent_idx_replaced': sent_idx_replaced}\n",
        "                        for ind, (fake_summary, sent_idx_replaced) in zip(sentence_replacement_idx, sent_results)]\n",
        "      \n",
        "      word_results = []\n",
        "      for reference_summaries in tqdm(chunker(word_replacement_indexed_summaries, chunk_size=num_threads), total=len(word_replacement_indexed_summaries)/num_threads):\n",
        "        word_results += pool.map(generate_word_replacement_summary, reference_summaries)\n",
        "      word_summaries = [{'ds_sample_id': int(ind), 'summary':' \u003cSNT\u003e '.join(fake_summary), 'sent_idx_replaced': sent_idx_replaced}\n",
        "                        for ind, (fake_summary, sent_idx_replaced) in zip(word_replacement_idx, word_results)]\n",
        "      json.dump(sentence_summaries, sw)\n",
        "      json.dump(word_summaries, ww)\n",
        "      json.dump(pos_summaries, pw)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "nJvoSxFrC0Tp"
      },
      "cell_type": "code",
      "source": [
        "# The following functions can be used to convert a value to a type compatible\n",
        "# with tf.train.Example.\n",
        "\n",
        "def _bytes_feature(value):\n",
        "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "  if isinstance(value, type(tf.constant(0))):\n",
        "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def _float_feature(value):\n",
        "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "def _int64_feature(value):\n",
        "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
        "def _float_array_feature(value):\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
        "\n",
        "@tf.py_function(Tout=tf.string)\n",
        "def serialize_example(idx, documents, summary, aggregate_label, instance_labels):\n",
        "  \"\"\"\n",
        "  Creates a tf.train.Example message ready to be written to a file.\n",
        "  \"\"\"\n",
        "  feature = {\n",
        "      'id': _int64_feature(idx),\n",
        "      'documents': _bytes_feature(documents),\n",
        "      'summary': _bytes_feature(summary),\n",
        "      'aggregate_label': _float_feature(aggregate_label),\n",
        "      'instance_labels': _float_array_feature(instance_labels),\n",
        "  }\n",
        "\n",
        "  # Create a Features message using tf.train.Example.\n",
        "\n",
        "  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "  return example_proto.SerializeToString()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "RvFDaprWC4FZ"
      },
      "cell_type": "code",
      "source": [
        "# Defining SeqIO Task for Synthetic Dataset\n",
        "domains = ['animal', 'company', 'film']\n",
        "splits = ['test','valid','train']\n",
        "\n",
        "# Writing TFRecord file\n",
        "tfrecord_filenames = {'train':'train.tfrecord', 'test':'test.tfrecord', 'valid':'valid.tfrecord'}\n",
        "\n",
        "# Write the `tf.train.Example` observations to the file.\n",
        "for split in tqdm(splits):\n",
        "  filename_output = f'{data_dir}/{tfrecord_filenames[split]}'\n",
        "  with tf.io.TFRecordWriter(filename_output) as writer:\n",
        "    for domain in tqdm(domains):\n",
        "      print('domain:', domain, 'split:', split)\n",
        "      sentence_samples_file = f'{data_dir}/{domain}/fake_sentence_replacement_{split}.tgt.json'\n",
        "      word_samples_file = f'{data_dir}/{domain}/fake_word_replacement_{split}.tgt.json'\n",
        "      pos_samples_file = f'{data_dir}/{domain}/pos_{split}.tgt.json'\n",
        "      document_file = f'{data_dir}/{domain}/{split}.src'\n",
        "      with open(document_file , 'r') as df:\n",
        "        df_readlines = df.readlines()\n",
        "        sf = np.array(json.load(open(sentence_samples_file , 'r')))\n",
        "        wf = np.array(json.load(open(word_samples_file , 'r')))\n",
        "        pf = np.array(json.load(open(pos_samples_file , 'r')))\n",
        "        summary_types_dict = {'sentence': sf, 'word': wf, 'pos': pf}\n",
        "        for summary_type in summary_types_dict.keys():\n",
        "          agg_label = 1\n",
        "          for sample in summary_types_dict[summary_type]:\n",
        "            if summary_type == 'sentence' or summary_type == 'word':\n",
        "              agg_label = 0\n",
        "            else:\n",
        "              agg_label = 1\n",
        "            instance_labels = np.ones(len(sample['summary'].split('\u003cSNT\u003e')))\n",
        "            if len(sample['sent_idx_replaced']) \u003e 0:\n",
        "              instance_labels[np.array(sample['sent_idx_replaced'])] = 0\n",
        "            else:\n",
        "              agg_label = 1\n",
        "            example = serialize_example(sample['ds_sample_id'], df_readlines[sample['ds_sample_id']], sample['summary'], agg_label, instance_labels)\n",
        "            writer.write(example.numpy())\n",
        "\n",
        "\n",
        "# sentence_output_file = f'{data_dir}/{domain}/fake_sentence_replacement_{split}.tgt.json'\n",
        "# word_output_file = f'{data_dir}/{domain}/fake_word_replacement_{split}.tgt.json'\n",
        "# pos_output_file = f'{data_dir}/{domain}/pos_{split}.tgt.json'"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "PXw8vBkIC9qY"
      },
      "cell_type": "code",
      "source": [
        "feature_description = {\n",
        "  'id': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
        "  'documents': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "  'summary': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "  'aggregate_label': tf.io.FixedLenFeature([], tf.float32, default_value=0.0),\n",
        "  'instance_labels': tf.io.RaggedFeature(tf.float32),\n",
        "}\n",
        "\n",
        "def _bytes_feature(value):\n",
        "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "  if isinstance(value, type(tf.constant(0))):\n",
        "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def _parse_function(example_proto):\n",
        "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
        "  return tf.io.parse_single_example(example_proto, feature_description)\n",
        "\n",
        "def process_dataset(dataset_path, encoder_type = 'sentence-t5', aggregation_type = 'min'):\n",
        "  encoder = None\n",
        "  if encoder_type == 'sentence-t5':\n",
        "    sentence_t5_url = \"https://tfhub.dev/google/sentence-t5/st5-large/1\"\n",
        "    encoder = hub.KerasLayer(sentence_t5_url)\n",
        "  # tfrecord_filenames = {'train':f'{dataset_path}/train.tfrecord', 'test':f'{dataset_path}/test.tfrecord', 'valid':f'{dataset_path}/valid.tfrecord'}\n",
        "  tfrecord_filenames = {'train':f'{dataset_path}/train.tfrecord'}\n",
        "  raw_dataset = {}\n",
        "  parsed_dataset = {}\n",
        "  for filename in tfrecord_filenames.keys():\n",
        "    raw_dataset[filename] = tf.data.TFRecordDataset(tfrecord_filenames[filename])\n",
        "    parsed_dataset[filename] = raw_dataset[filename].map(_parse_function)\n",
        "    num_document_splits = [] # Number of splits for a document\n",
        "    num_summary_sentences = [] # Number of instances in a bag\n",
        "    document_splits = [] # Document splits for the entire dataset\n",
        "    summary_sentences = [] # Summary sentences for the entire dataset\n",
        "    target_agg_score = []\n",
        "    target_instance_score = []\n",
        "\n",
        "    for p in tqdm(parsed_dataset[filename]):\n",
        "      sample_summary_sentences = p['summary'].numpy().decode(\"utf-8\").split('\u003cSNT\u003e')\n",
        "      num_summary_sentences.append(len(sample_summary_sentences))\n",
        "      summary_sentences += sample_summary_sentences\n",
        "\n",
        "      document = p['documents'].numpy().decode(\"utf-8\").split()\n",
        "      document_split_sample = [' '.join(document[x:x+1000]) for x in range(0, len(document), 1000)]\n",
        "      num_document_splits.append(len(document_split_sample))\n",
        "      document_splits += document_split_sample\n",
        "\n",
        "      sample_instance_score = list(p['instance_labels'].numpy())\n",
        "      assert len(sample_instance_score) == len(sample_summary_sentences)\n",
        "      target_instance_score += sample_instance_score\n",
        "\n",
        "      sample_agg_score = p['aggregate_label'].numpy()\n",
        "      target_agg_score.append(sample_agg_score)\n",
        "\n",
        "    document_splits_embedding = tf.concat([encoder(tf.constant(document_splits[x:x+500]))[0] for x in range(0, len(document_splits), 500)], axis=0)\n",
        "    summary_sentences_embedding = tf.concat([encoder(tf.constant(summary_sentences[x:x+500]))[0] for x in range(0, len(summary_sentences), 500)], axis=0)\n",
        "    summary_sentences_embedding = tf.RaggedTensor.from_row_lengths(summary_sentences_embedding, num_summary_sentences)\n",
        "    document_splits_embedding = tf.RaggedTensor.from_row_lengths(document_splits_embedding, num_document_splits)\n",
        "    target_instance_score = tf.RaggedTensor.from_row_lengths(target_instance_score, num_summary_sentences)\n",
        "    ds = tf.data.Dataset.from_tensor_slices((document_splits_embedding, summary_sentences_embedding, num_document_splits, num_summary_sentences, target_agg_score, target_instance_score))\n",
        "    ds = ds.shuffle(len(ds))\n",
        "    ds.save(f'{data_dir}/{encoder_type}_{filename}')\n",
        "    \n",
        "\n",
        "\n",
        "process_dataset(data_dir, encoder_type = 'sentence-t5', aggregation_type = 'min')"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
